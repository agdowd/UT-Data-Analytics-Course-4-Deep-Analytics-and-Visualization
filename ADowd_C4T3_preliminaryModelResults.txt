C5.0 

3996 samples
 200 predictor
 259 classes: 'A_0_102_2', 'A_0_106_2', 'A_0_107_2', 'A_0_110_2', 'A_0_111_2', 'A_0_112_2', 'A_0_113_2', 'A_0_114_2', 'A_0_115_2', 'A_0_116_2', 'A_0_117_2', 'A_0_118_2', 'A_0_119_2', 'A_0_120_2', 'A_0_121_2', 'A_0_122_2', 'A_0_123_2', 'A_0_125_2', 'A_0_126_2', 'A_0_127_2', 'A_0_128_2', 'A_0_129_2', 'A_0_130_2', 'A_0_131_2', 'A_0_132_2', 'A_0_133_2', 'A_0_134_2', 'A_0_201_2', 'A_0_202_2', 'A_0_208_2', 'A_0_209_2', 'A_0_211_2', 'A_0_212_2', 'A_0_213_2', 'A_0_214_2', 'A_0_215_2', 'A_0_216_2', 'A_0_218_2', 'A_0_219_2', 'A_0_220_2', 'A_0_222_2', 'A_0_224_2', 'A_0_225_2', 'A_0_226_2', 'A_0_227_2', 'A_0_229_2', 'A_0_230_2', 'A_0_231_2', 'A_0_232_2', 'A_0_233_2', 'A_0_234_2', 'A_0_235_2', 'A_0_236_2', 'A_0_237_2', 'A_1_101_2', 'A_1_102_2', 'A_1_103_2', 'A_1_104_2', 'A_1_105_2', 'A_1_106_2', 'A_1_107_2', 'A_1_108_2', 'A_1_109_2', 'A_1_110_2', 'A_1_111_2', 'A_1_112_2', 'A_1_113_2', 'A_1_114_2', 'A_1_115_2', 'A_1_116_2', 'A_1_117_2', 'A_1_118_2', 'A_1_119_2', 'A_1_120_2', 'A_1_121_2', 'A_1_122_2', 'A_1_123_2', 'A_1_124_2', 'A_1_125_2', 'A_1_126_2', 'A_1_127_2', 'A_1_128_2', 'A_1_129_2', 'A_1_130_2', 'A_1_136_2', 'A_1_137_2', 'A_1_138_2', 'A_1_201_2', 'A_1_202_2', 'A_1_203_2', 'A_1_204_2', 'A_1_205_1', 'A_1_205_2', 'A_1_206_2', 'A_1_207_2', 'A_1_208_2', 'A_1_209_2', 'A_1_210_2', 'A_1_211_2', 'A_1_212_2', 'A_1_213_2', 'A_1_214_2', 'A_1_215_2', 'A_1_216_2', 'A_1_217_2', 'A_1_218_2', 'A_1_219_2', 'A_1_220_2', 'A_1_221_2', 'A_1_222_2', 'A_1_223_2', 'A_1_224_2', 'A_1_225_2', 'A_1_226_2', 'A_1_227_2', 'A_1_228_2', 'A_1_229_2', 'A_1_230_2', 'A_1_233_2', 'A_1_234_2', 'A_1_235_2', 'A_2_101_2', 'A_2_102_2', 'A_2_103_2', 'A_2_104_2', 'A_2_105_2', 'A_2_106_2', 'A_2_107_2', 'A_2_108_2', 'A_2_109_2', 'A_2_110_2', 'A_2_111_2', 'A_2_112_2', 'A_2_113_2', 'A_2_114_2', 'A_2_115_2', 'A_2_117_2', 'A_2_118_2', 'A_2_119_2', 'A_2_120_2', 'A_2_121_2', 'A_2_122_2', 'A_2_123_2', 'A_2_124_2', 'A_2_125_2', 'A_2_126_2', 'A_2_127_2', 'A_2_128_1', 'A_2_128_2', 'A_2_129_2', 'A_2_130_2', 'A_2_132_2', 'A_2_133_2', 'A_2_134_2', 'A_2_138_2', 'A_2_139_2', 'A_2_140_2', 'A_2_201_2', 'A_2_202_2', 'A_2_203_2', 'A_2_204_2', 'A_2_205_2', 'A_2_206_2', 'A_2_207_2', 'A_2_208_2', 'A_2_209_2', 'A_2_210_2', 'A_2_211_2', 'A_2_212_2', 'A_2_213_2', 'A_2_214_1', 'A_2_214_2', 'A_2_216_2', 'A_2_217_2', 'A_2_218_2', 'A_2_219_2', 'A_2_220_2', 'A_2_221_2', 'A_2_222_2', 'A_2_223_2', 'A_2_224_2', 'A_2_225_2', 'A_2_226_2', 'A_2_227_2', 'A_2_228_2', 'A_2_229_2', 'A_2_230_2', 'A_2_231_2', 'A_2_234_2', 'A_2_235_2', 'A_2_241_2', 'A_3_101_2', 'A_3_102_2', 'A_3_103_2', 'A_3_104_2', 'A_3_105_2', 'A_3_106_2', 'A_3_107_2', 'A_3_108_2', 'A_3_109_2', 'A_3_110_2', 'A_3_111_2', 'A_3_112_2', 'A_3_113_2', 'A_3_114_2', 'A_3_115_2', 'A_3_116_2', 'A_3_117_2', 'A_3_118_2', 'A_3_119_2', 'A_3_120_2', 'A_3_121_2', 'A_3_122_2', 'A_3_123_2', 'A_3_124_2', 'A_3_125_2', 'A_3_126_2', 'A_3_127_2', 'A_3_128_2', 'A_3_129_2', 'A_3_130_2', 'A_3_131_2', 'A_3_135_2', 'A_3_136_2', 'A_3_137_2', 'A_3_201_2', 'A_3_202_2', 'A_3_203_2', 'A_3_204_2', 'A_3_205_2', 'A_3_206_2', 'A_3_207_2', 'A_3_208_2', 'A_3_209_2', 'A_3_210_2', 'A_3_211_2', 'A_3_212_2', 'A_3_213_2', 'A_3_214_2', 'A_3_215_2', 'A_3_216_2', 'A_3_217_2', 'A_3_218_2', 'A_3_219_2', 'A_3_220_2', 'A_3_221_2', 'A_3_222_2', 'A_3_223_2', 'A_3_224_2', 'A_3_225_2', 'A_3_226_2', 'A_3_227_2', 'A_3_228_2', 'A_3_229_2', 'A_3_230_2', 'A_3_231_2', 'A_3_234_2', 'A_3_235_2', 'A_3_236_2' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 3602, 3601, 3599, 3601, 3584, 3593, ... 
Resampling results across tuning parameters:

  model  winnow  trials  Accuracy   Kappa    
  rules  FALSE    1      0.5636434  0.5617884
  rules  FALSE   10      0.6823919  0.6810334
  rules  FALSE   20      0.7093248  0.7080792
  rules  FALSE   30      0.7108108  0.7095714
  rules  FALSE   40      0.7175642  0.7163533
  rules  FALSE   50      0.7168145  0.7155999
  rules  FALSE   60      0.7168510  0.7156348
  rules  FALSE   70      0.7195952  0.7183910
  rules  FALSE   80      0.7188445  0.7176373
  rules  FALSE   90      0.7205785  0.7193776
  rules   TRUE    1      0.5632627  0.5614068
  rules   TRUE   10      0.6856044  0.6842586
  rules   TRUE   20      0.7062947  0.7050381
  rules   TRUE   30      0.7130526  0.7118219
  rules   TRUE   40      0.7156079  0.7143904
  rules   TRUE   50      0.7158951  0.7146788
  rules   TRUE   60      0.7191605  0.7179580
  rules   TRUE   70      0.7218829  0.7206901
  rules   TRUE   80      0.7213387  0.7201445
  rules   TRUE   90      0.7188001  0.7175938
  tree   FALSE    1      0.5748550  0.5730579
  tree   FALSE   10      0.6791968  0.6778239
  tree   FALSE   20      0.6982727  0.6969757
  tree   FALSE   30      0.7034419  0.7021639
  tree   FALSE   40      0.7055255  0.7042599
  tree   FALSE   50      0.7087485  0.7074972
  tree   FALSE   60      0.7123047  0.7110677
  tree   FALSE   70      0.7115378  0.7102963
  tree   FALSE   80      0.7143030  0.7130724
  tree   FALSE   90      0.7145901  0.7133632
  tree    TRUE    1      0.5656817  0.5638394
  tree    TRUE   10      0.6801693  0.6788089
  tree    TRUE   20      0.7013641  0.7000872
  tree    TRUE   30      0.7055758  0.7043176
  tree    TRUE   40      0.7145478  0.7133224
  tree    TRUE   50      0.7132784  0.7120500
  tree    TRUE   60      0.7125521  0.7113200
  tree    TRUE   70      0.7123048  0.7110737
  tree    TRUE   80      0.7152525  0.7140330
  tree    TRUE   90      0.7137475  0.7125215

Accuracy was used to select the optimal model using the largest value.
The final values used for the model were trials = 70, model = rules and winnow = TRUE.


############################################################################################


Bagged AdaBoost 

3996 samples
 200 predictor
 259 classes: '0_0_102_2', '0_0_106_2', '0_0_107_2', '0_0_110_2', '0_0_111_2', '0_0_112_2', '0_0_113_2', '0_0_114_2', '0_0_115_2', '0_0_116_2', '0_0_117_2', '0_0_118_2', '0_0_119_2', '0_0_120_2', '0_0_121_2', '0_0_122_2', '0_0_123_2', '0_0_125_2', '0_0_126_2', '0_0_127_2', '0_0_128_2', '0_0_129_2', '0_0_130_2', '0_0_131_2', '0_0_132_2', '0_0_133_2', '0_0_134_2', '0_0_201_2', '0_0_202_2', '0_0_208_2', '0_0_209_2', '0_0_211_2', '0_0_212_2', '0_0_213_2', '0_0_214_2', '0_0_215_2', '0_0_216_2', '0_0_218_2', '0_0_219_2', '0_0_220_2', '0_0_222_2', '0_0_224_2', '0_0_225_2', '0_0_226_2', '0_0_227_2', '0_0_229_2', '0_0_230_2', '0_0_231_2', '0_0_232_2', '0_0_233_2', '0_0_234_2', '0_0_235_2', '0_0_236_2', '0_0_237_2', '0_1_101_2', '0_1_102_2', '0_1_103_2', '0_1_104_2', '0_1_105_2', '0_1_106_2', '0_1_107_2', '0_1_108_2', '0_1_109_2', '0_1_110_2', '0_1_111_2', '0_1_112_2', '0_1_113_2', '0_1_114_2', '0_1_115_2', '0_1_116_2', '0_1_117_2', '0_1_118_2', '0_1_119_2', '0_1_120_2', '0_1_121_2', '0_1_122_2', '0_1_123_2', '0_1_124_2', '0_1_125_2', '0_1_126_2', '0_1_127_2', '0_1_128_2', '0_1_129_2', '0_1_130_2', '0_1_136_2', '0_1_137_2', '0_1_138_2', '0_1_201_2', '0_1_202_2', '0_1_203_2', '0_1_204_2', '0_1_205_1', '0_1_205_2', '0_1_206_2', '0_1_207_2', '0_1_208_2', '0_1_209_2', '0_1_210_2', '0_1_211_2', '0_1_212_2', '0_1_213_2', '0_1_214_2', '0_1_215_2', '0_1_216_2', '0_1_217_2', '0_1_218_2', '0_1_219_2', '0_1_220_2', '0_1_221_2', '0_1_222_2', '0_1_223_2', '0_1_224_2', '0_1_225_2', '0_1_226_2', '0_1_227_2', '0_1_228_2', '0_1_229_2', '0_1_230_2', '0_1_233_2', '0_1_234_2', '0_1_235_2', '0_2_101_2', '0_2_102_2', '0_2_103_2', '0_2_104_2', '0_2_105_2', '0_2_106_2', '0_2_107_2', '0_2_108_2', '0_2_109_2', '0_2_110_2', '0_2_111_2', '0_2_112_2', '0_2_113_2', '0_2_114_2', '0_2_115_2', '0_2_117_2', '0_2_118_2', '0_2_119_2', '0_2_120_2', '0_2_121_2', '0_2_122_2', '0_2_123_2', '0_2_124_2', '0_2_125_2', '0_2_126_2', '0_2_127_2', '0_2_128_1', '0_2_128_2', '0_2_129_2', '0_2_130_2', '0_2_132_2', '0_2_133_2', '0_2_134_2', '0_2_138_2', '0_2_139_2', '0_2_140_2', '0_2_201_2', '0_2_202_2', '0_2_203_2', '0_2_204_2', '0_2_205_2', '0_2_206_2', '0_2_207_2', '0_2_208_2', '0_2_209_2', '0_2_210_2', '0_2_211_2', '0_2_212_2', '0_2_213_2', '0_2_214_1', '0_2_214_2', '0_2_216_2', '0_2_217_2', '0_2_218_2', '0_2_219_2', '0_2_220_2', '0_2_221_2', '0_2_222_2', '0_2_223_2', '0_2_224_2', '0_2_225_2', '0_2_226_2', '0_2_227_2', '0_2_228_2', '0_2_229_2', '0_2_230_2', '0_2_231_2', '0_2_234_2', '0_2_235_2', '0_2_241_2', '0_3_101_2', '0_3_102_2', '0_3_103_2', '0_3_104_2', '0_3_105_2', '0_3_106_2', '0_3_107_2', '0_3_108_2', '0_3_109_2', '0_3_110_2', '0_3_111_2', '0_3_112_2', '0_3_113_2', '0_3_114_2', '0_3_115_2', '0_3_116_2', '0_3_117_2', '0_3_118_2', '0_3_119_2', '0_3_120_2', '0_3_121_2', '0_3_122_2', '0_3_123_2', '0_3_124_2', '0_3_125_2', '0_3_126_2', '0_3_127_2', '0_3_128_2', '0_3_129_2', '0_3_130_2', '0_3_131_2', '0_3_135_2', '0_3_136_2', '0_3_137_2', '0_3_201_2', '0_3_202_2', '0_3_203_2', '0_3_204_2', '0_3_205_2', '0_3_206_2', '0_3_207_2', '0_3_208_2', '0_3_209_2', '0_3_210_2', '0_3_211_2', '0_3_212_2', '0_3_213_2', '0_3_214_2', '0_3_215_2', '0_3_216_2', '0_3_217_2', '0_3_218_2', '0_3_219_2', '0_3_220_2', '0_3_221_2', '0_3_222_2', '0_3_223_2', '0_3_224_2', '0_3_225_2', '0_3_226_2', '0_3_227_2', '0_3_228_2', '0_3_229_2', '0_3_230_2', '0_3_231_2', '0_3_234_2', '0_3_235_2', '0_3_236_2' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 3602, 3601, 3599, 3601, 3584, 3593, ... 
Resampling results across tuning parameters:

  maxdepth  mfinal  Accuracy     Kappa       
   1         50     0.008503938  6.205581e-04
   1        100     0.008007660  6.557850e-07
   1        150     0.008007660  6.557850e-07
   1        200     0.008007660  5.934401e-07
   1        250     0.008007660  6.557850e-07
   1        300     0.008007660  6.557850e-07
   1        350     0.008007660  1.186873e-06
   1        400     0.008007660  5.934401e-07
   1        450     0.008007660  0.000000e+00
   1        500     0.008007660  0.000000e+00
   2         50     0.018282153  1.132318e-02
   2        100     0.018269807  1.109165e-02
   2        150     0.017018325  9.708148e-03
   2        200     0.017017030  9.707606e-03
   2        250     0.016758741  9.436081e-03
   2        300     0.017014344  9.711089e-03
   2        350     0.017521958  1.022488e-02
   2        400     0.017022451  9.718445e-03
   2        450     0.017024388  9.724818e-03
   2        500     0.017758797  1.051739e-02
   3         50     0.034215423  2.802495e-02
   3        100     0.033210578  2.681987e-02
   3        150     0.033455754  2.709805e-02
   3        200     0.035205579  2.883740e-02
   3        250     0.035494503  2.911663e-02
   3        300     0.035467469  2.905993e-02
   3        350     0.035965722  2.959199e-02
   3        400     0.035718229  2.934078e-02
   3        450     0.034297822  2.794310e-02
   3        500     0.032768505  2.640719e-02
   4         50     0.068980435  6.363325e-02
   4        100     0.073111557  6.760407e-02
   4        150     0.078858271  7.341595e-02
   4        200     0.078081098  7.259907e-02
   4        250     0.079543243  7.407877e-02
   4        300     0.084020735  7.862113e-02
   4        350     0.085038141  7.963295e-02
   4        400     0.085283771  7.985724e-02
   4        450     0.087547202  8.215590e-02
   4        500     0.085559953  8.015510e-02
   5         50     0.124245500  1.194064e-01
   5        100     0.136050868  1.312911e-01
   5        150     0.137584599  1.328427e-01
   5        200     0.142597093  1.378569e-01
   5        250     0.145666075  1.409411e-01
   5        300     0.146663360  1.419441e-01
   5        350     0.147398857  1.426949e-01
   5        400     0.146415471  1.417211e-01
   5        450     0.149173142  1.444904e-01
   5        500     0.147571743  1.428766e-01
   6         50     0.196705716  1.924860e-01
   6        100     0.206596998  2.023804e-01
   6        150     0.207164255  2.029530e-01
   6        200     0.204149328  1.999062e-01
   6        250     0.208935641  2.047408e-01
   6        300     0.208677135  2.044830e-01
   6        350     0.207964750  2.037613e-01
   6        400     0.208704666  2.045082e-01
   6        450     0.205933716  2.017191e-01
   6        500     0.208188648  2.039744e-01
   7         50     0.269419446  2.656329e-01
   7        100     0.281087933  2.773477e-01
   7        150     0.284905403  2.811777e-01
   7        200     0.285407287  2.816846e-01
   7        250     0.287202618  2.834934e-01
   7        300     0.291183272  2.875070e-01
   7        350     0.289895135  2.862130e-01
   7        400     0.293670485  2.900026e-01
   7        450     0.293901877  2.902427e-01
   7        500     0.293620181  2.899645e-01
   8         50     0.338750625  3.354626e-01
   8        100     0.362925894  3.597653e-01
   8        150     0.367604012  3.644584e-01
   8        200     0.368068263  3.649322e-01
   8        250     0.372041444  3.689231e-01
   8        300     0.371755240  3.686384e-01
   8        350     0.370747871  3.676185e-01
   8        400     0.372556666  3.694286e-01
   8        450     0.369526463  3.663829e-01
   8        500     0.371086413  3.679538e-01
   9         50     0.412881939  4.100679e-01
   9        100     0.434341028  4.316015e-01
   9        150     0.439845034  4.371299e-01
   9        200     0.444879479  4.421906e-01
   9        250     0.447149211  4.444730e-01
   9        300     0.450663125  4.479980e-01
   9        350     0.449681051  4.470211e-01
   9        400     0.450168274  4.475092e-01
   9        450     0.449947194  4.472900e-01
   9        500     0.450405863  4.477471e-01
  10         50     0.496945845  4.946219e-01
  10        100     0.510939901  5.086662e-01
  10        150     0.517687292  5.154427e-01
  10        200     0.521948956  5.197185e-01
  10        250     0.517760975  5.155050e-01
  10        300     0.521719652  5.194863e-01
  10        350     0.523784865  5.215644e-01
  10        400     0.520804275  5.185676e-01
  10        450     0.523583941  5.213607e-01
  10        500     0.525070376  5.228519e-01

Accuracy was used to select the optimal model using the largest value.
The final values used for the model were mfinal = 500 and maxdepth = 10.


############################################################################################


k-Nearest Neighbors 

3996 samples
 200 predictor
 259 classes: '0_0_102_2', '0_0_106_2', '0_0_107_2', '0_0_110_2', '0_0_111_2', '0_0_112_2', '0_0_113_2', '0_0_114_2', '0_0_115_2', '0_0_116_2', '0_0_117_2', '0_0_118_2', '0_0_119_2', '0_0_120_2', '0_0_121_2', '0_0_122_2', '0_0_123_2', '0_0_125_2', '0_0_126_2', '0_0_127_2', '0_0_128_2', '0_0_129_2', '0_0_130_2', '0_0_131_2', '0_0_132_2', '0_0_133_2', '0_0_134_2', '0_0_201_2', '0_0_202_2', '0_0_208_2', '0_0_209_2', '0_0_211_2', '0_0_212_2', '0_0_213_2', '0_0_214_2', '0_0_215_2', '0_0_216_2', '0_0_218_2', '0_0_219_2', '0_0_220_2', '0_0_222_2', '0_0_224_2', '0_0_225_2', '0_0_226_2', '0_0_227_2', '0_0_229_2', '0_0_230_2', '0_0_231_2', '0_0_232_2', '0_0_233_2', '0_0_234_2', '0_0_235_2', '0_0_236_2', '0_0_237_2', '0_1_101_2', '0_1_102_2', '0_1_103_2', '0_1_104_2', '0_1_105_2', '0_1_106_2', '0_1_107_2', '0_1_108_2', '0_1_109_2', '0_1_110_2', '0_1_111_2', '0_1_112_2', '0_1_113_2', '0_1_114_2', '0_1_115_2', '0_1_116_2', '0_1_117_2', '0_1_118_2', '0_1_119_2', '0_1_120_2', '0_1_121_2', '0_1_122_2', '0_1_123_2', '0_1_124_2', '0_1_125_2', '0_1_126_2', '0_1_127_2', '0_1_128_2', '0_1_129_2', '0_1_130_2', '0_1_136_2', '0_1_137_2', '0_1_138_2', '0_1_201_2', '0_1_202_2', '0_1_203_2', '0_1_204_2', '0_1_205_1', '0_1_205_2', '0_1_206_2', '0_1_207_2', '0_1_208_2', '0_1_209_2', '0_1_210_2', '0_1_211_2', '0_1_212_2', '0_1_213_2', '0_1_214_2', '0_1_215_2', '0_1_216_2', '0_1_217_2', '0_1_218_2', '0_1_219_2', '0_1_220_2', '0_1_221_2', '0_1_222_2', '0_1_223_2', '0_1_224_2', '0_1_225_2', '0_1_226_2', '0_1_227_2', '0_1_228_2', '0_1_229_2', '0_1_230_2', '0_1_233_2', '0_1_234_2', '0_1_235_2', '0_2_101_2', '0_2_102_2', '0_2_103_2', '0_2_104_2', '0_2_105_2', '0_2_106_2', '0_2_107_2', '0_2_108_2', '0_2_109_2', '0_2_110_2', '0_2_111_2', '0_2_112_2', '0_2_113_2', '0_2_114_2', '0_2_115_2', '0_2_117_2', '0_2_118_2', '0_2_119_2', '0_2_120_2', '0_2_121_2', '0_2_122_2', '0_2_123_2', '0_2_124_2', '0_2_125_2', '0_2_126_2', '0_2_127_2', '0_2_128_1', '0_2_128_2', '0_2_129_2', '0_2_130_2', '0_2_132_2', '0_2_133_2', '0_2_134_2', '0_2_138_2', '0_2_139_2', '0_2_140_2', '0_2_201_2', '0_2_202_2', '0_2_203_2', '0_2_204_2', '0_2_205_2', '0_2_206_2', '0_2_207_2', '0_2_208_2', '0_2_209_2', '0_2_210_2', '0_2_211_2', '0_2_212_2', '0_2_213_2', '0_2_214_1', '0_2_214_2', '0_2_216_2', '0_2_217_2', '0_2_218_2', '0_2_219_2', '0_2_220_2', '0_2_221_2', '0_2_222_2', '0_2_223_2', '0_2_224_2', '0_2_225_2', '0_2_226_2', '0_2_227_2', '0_2_228_2', '0_2_229_2', '0_2_230_2', '0_2_231_2', '0_2_234_2', '0_2_235_2', '0_2_241_2', '0_3_101_2', '0_3_102_2', '0_3_103_2', '0_3_104_2', '0_3_105_2', '0_3_106_2', '0_3_107_2', '0_3_108_2', '0_3_109_2', '0_3_110_2', '0_3_111_2', '0_3_112_2', '0_3_113_2', '0_3_114_2', '0_3_115_2', '0_3_116_2', '0_3_117_2', '0_3_118_2', '0_3_119_2', '0_3_120_2', '0_3_121_2', '0_3_122_2', '0_3_123_2', '0_3_124_2', '0_3_125_2', '0_3_126_2', '0_3_127_2', '0_3_128_2', '0_3_129_2', '0_3_130_2', '0_3_131_2', '0_3_135_2', '0_3_136_2', '0_3_137_2', '0_3_201_2', '0_3_202_2', '0_3_203_2', '0_3_204_2', '0_3_205_2', '0_3_206_2', '0_3_207_2', '0_3_208_2', '0_3_209_2', '0_3_210_2', '0_3_211_2', '0_3_212_2', '0_3_213_2', '0_3_214_2', '0_3_215_2', '0_3_216_2', '0_3_217_2', '0_3_218_2', '0_3_219_2', '0_3_220_2', '0_3_221_2', '0_3_222_2', '0_3_223_2', '0_3_224_2', '0_3_225_2', '0_3_226_2', '0_3_227_2', '0_3_228_2', '0_3_229_2', '0_3_230_2', '0_3_231_2', '0_3_234_2', '0_3_235_2', '0_3_236_2' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 3602, 3601, 3599, 3601, 3584, 3593, ... 
Resampling results across tuning parameters:

  k   Accuracy   Kappa    
   5  0.5430954  0.5411717
   7  0.5243082  0.5223035
   9  0.5001019  0.4979978
  11  0.4868470  0.4847010
  13  0.4603488  0.4580968
  15  0.4327857  0.4304276
  17  0.4175750  0.4151647
  19  0.3940204  0.3915133
  21  0.3749816  0.3723941
  23  0.3575864  0.3549200

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was k = 5.


############################################################################################


k-Nearest Neighbors 

3996 samples
 200 predictor
 259 classes: '0_0_102_2', '0_0_106_2', '0_0_107_2', '0_0_110_2', '0_0_111_2', '0_0_112_2', '0_0_113_2', '0_0_114_2', '0_0_115_2', '0_0_116_2', '0_0_117_2', '0_0_118_2', '0_0_119_2', '0_0_120_2', '0_0_121_2', '0_0_122_2', '0_0_123_2', '0_0_125_2', '0_0_126_2', '0_0_127_2', '0_0_128_2', '0_0_129_2', '0_0_130_2', '0_0_131_2', '0_0_132_2', '0_0_133_2', '0_0_134_2', '0_0_201_2', '0_0_202_2', '0_0_208_2', '0_0_209_2', '0_0_211_2', '0_0_212_2', '0_0_213_2', '0_0_214_2', '0_0_215_2', '0_0_216_2', '0_0_218_2', '0_0_219_2', '0_0_220_2', '0_0_222_2', '0_0_224_2', '0_0_225_2', '0_0_226_2', '0_0_227_2', '0_0_229_2', '0_0_230_2', '0_0_231_2', '0_0_232_2', '0_0_233_2', '0_0_234_2', '0_0_235_2', '0_0_236_2', '0_0_237_2', '0_1_101_2', '0_1_102_2', '0_1_103_2', '0_1_104_2', '0_1_105_2', '0_1_106_2', '0_1_107_2', '0_1_108_2', '0_1_109_2', '0_1_110_2', '0_1_111_2', '0_1_112_2', '0_1_113_2', '0_1_114_2', '0_1_115_2', '0_1_116_2', '0_1_117_2', '0_1_118_2', '0_1_119_2', '0_1_120_2', '0_1_121_2', '0_1_122_2', '0_1_123_2', '0_1_124_2', '0_1_125_2', '0_1_126_2', '0_1_127_2', '0_1_128_2', '0_1_129_2', '0_1_130_2', '0_1_136_2', '0_1_137_2', '0_1_138_2', '0_1_201_2', '0_1_202_2', '0_1_203_2', '0_1_204_2', '0_1_205_1', '0_1_205_2', '0_1_206_2', '0_1_207_2', '0_1_208_2', '0_1_209_2', '0_1_210_2', '0_1_211_2', '0_1_212_2', '0_1_213_2', '0_1_214_2', '0_1_215_2', '0_1_216_2', '0_1_217_2', '0_1_218_2', '0_1_219_2', '0_1_220_2', '0_1_221_2', '0_1_222_2', '0_1_223_2', '0_1_224_2', '0_1_225_2', '0_1_226_2', '0_1_227_2', '0_1_228_2', '0_1_229_2', '0_1_230_2', '0_1_233_2', '0_1_234_2', '0_1_235_2', '0_2_101_2', '0_2_102_2', '0_2_103_2', '0_2_104_2', '0_2_105_2', '0_2_106_2', '0_2_107_2', '0_2_108_2', '0_2_109_2', '0_2_110_2', '0_2_111_2', '0_2_112_2', '0_2_113_2', '0_2_114_2', '0_2_115_2', '0_2_117_2', '0_2_118_2', '0_2_119_2', '0_2_120_2', '0_2_121_2', '0_2_122_2', '0_2_123_2', '0_2_124_2', '0_2_125_2', '0_2_126_2', '0_2_127_2', '0_2_128_1', '0_2_128_2', '0_2_129_2', '0_2_130_2', '0_2_132_2', '0_2_133_2', '0_2_134_2', '0_2_138_2', '0_2_139_2', '0_2_140_2', '0_2_201_2', '0_2_202_2', '0_2_203_2', '0_2_204_2', '0_2_205_2', '0_2_206_2', '0_2_207_2', '0_2_208_2', '0_2_209_2', '0_2_210_2', '0_2_211_2', '0_2_212_2', '0_2_213_2', '0_2_214_1', '0_2_214_2', '0_2_216_2', '0_2_217_2', '0_2_218_2', '0_2_219_2', '0_2_220_2', '0_2_221_2', '0_2_222_2', '0_2_223_2', '0_2_224_2', '0_2_225_2', '0_2_226_2', '0_2_227_2', '0_2_228_2', '0_2_229_2', '0_2_230_2', '0_2_231_2', '0_2_234_2', '0_2_235_2', '0_2_241_2', '0_3_101_2', '0_3_102_2', '0_3_103_2', '0_3_104_2', '0_3_105_2', '0_3_106_2', '0_3_107_2', '0_3_108_2', '0_3_109_2', '0_3_110_2', '0_3_111_2', '0_3_112_2', '0_3_113_2', '0_3_114_2', '0_3_115_2', '0_3_116_2', '0_3_117_2', '0_3_118_2', '0_3_119_2', '0_3_120_2', '0_3_121_2', '0_3_122_2', '0_3_123_2', '0_3_124_2', '0_3_125_2', '0_3_126_2', '0_3_127_2', '0_3_128_2', '0_3_129_2', '0_3_130_2', '0_3_131_2', '0_3_135_2', '0_3_136_2', '0_3_137_2', '0_3_201_2', '0_3_202_2', '0_3_203_2', '0_3_204_2', '0_3_205_2', '0_3_206_2', '0_3_207_2', '0_3_208_2', '0_3_209_2', '0_3_210_2', '0_3_211_2', '0_3_212_2', '0_3_213_2', '0_3_214_2', '0_3_215_2', '0_3_216_2', '0_3_217_2', '0_3_218_2', '0_3_219_2', '0_3_220_2', '0_3_221_2', '0_3_222_2', '0_3_223_2', '0_3_224_2', '0_3_225_2', '0_3_226_2', '0_3_227_2', '0_3_228_2', '0_3_229_2', '0_3_230_2', '0_3_231_2', '0_3_234_2', '0_3_235_2', '0_3_236_2' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 3602, 3601, 3599, 3601, 3584, 3593, ... 
Resampling results across tuning parameters:

  k  Accuracy   Kappa    
  1  0.5867336  0.5849830
  2  0.5549268  0.5530357
  3  0.5590661  0.5572023
  4  0.5508626  0.5489594
  5  0.5379065  0.5359558

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was k = 1.


############################################################################################


Random Forest 

3996 samples
 200 predictor
 259 classes: '0_0_102_2', '0_0_106_2', '0_0_107_2', '0_0_110_2', '0_0_111_2', '0_0_112_2', '0_0_113_2', '0_0_114_2', '0_0_115_2', '0_0_116_2', '0_0_117_2', '0_0_118_2', '0_0_119_2', '0_0_120_2', '0_0_121_2', '0_0_122_2', '0_0_123_2', '0_0_125_2', '0_0_126_2', '0_0_127_2', '0_0_128_2', '0_0_129_2', '0_0_130_2', '0_0_131_2', '0_0_132_2', '0_0_133_2', '0_0_134_2', '0_0_201_2', '0_0_202_2', '0_0_208_2', '0_0_209_2', '0_0_211_2', '0_0_212_2', '0_0_213_2', '0_0_214_2', '0_0_215_2', '0_0_216_2', '0_0_218_2', '0_0_219_2', '0_0_220_2', '0_0_222_2', '0_0_224_2', '0_0_225_2', '0_0_226_2', '0_0_227_2', '0_0_229_2', '0_0_230_2', '0_0_231_2', '0_0_232_2', '0_0_233_2', '0_0_234_2', '0_0_235_2', '0_0_236_2', '0_0_237_2', '0_1_101_2', '0_1_102_2', '0_1_103_2', '0_1_104_2', '0_1_105_2', '0_1_106_2', '0_1_107_2', '0_1_108_2', '0_1_109_2', '0_1_110_2', '0_1_111_2', '0_1_112_2', '0_1_113_2', '0_1_114_2', '0_1_115_2', '0_1_116_2', '0_1_117_2', '0_1_118_2', '0_1_119_2', '0_1_120_2', '0_1_121_2', '0_1_122_2', '0_1_123_2', '0_1_124_2', '0_1_125_2', '0_1_126_2', '0_1_127_2', '0_1_128_2', '0_1_129_2', '0_1_130_2', '0_1_136_2', '0_1_137_2', '0_1_138_2', '0_1_201_2', '0_1_202_2', '0_1_203_2', '0_1_204_2', '0_1_205_1', '0_1_205_2', '0_1_206_2', '0_1_207_2', '0_1_208_2', '0_1_209_2', '0_1_210_2', '0_1_211_2', '0_1_212_2', '0_1_213_2', '0_1_214_2', '0_1_215_2', '0_1_216_2', '0_1_217_2', '0_1_218_2', '0_1_219_2', '0_1_220_2', '0_1_221_2', '0_1_222_2', '0_1_223_2', '0_1_224_2', '0_1_225_2', '0_1_226_2', '0_1_227_2', '0_1_228_2', '0_1_229_2', '0_1_230_2', '0_1_233_2', '0_1_234_2', '0_1_235_2', '0_2_101_2', '0_2_102_2', '0_2_103_2', '0_2_104_2', '0_2_105_2', '0_2_106_2', '0_2_107_2', '0_2_108_2', '0_2_109_2', '0_2_110_2', '0_2_111_2', '0_2_112_2', '0_2_113_2', '0_2_114_2', '0_2_115_2', '0_2_117_2', '0_2_118_2', '0_2_119_2', '0_2_120_2', '0_2_121_2', '0_2_122_2', '0_2_123_2', '0_2_124_2', '0_2_125_2', '0_2_126_2', '0_2_127_2', '0_2_128_1', '0_2_128_2', '0_2_129_2', '0_2_130_2', '0_2_132_2', '0_2_133_2', '0_2_134_2', '0_2_138_2', '0_2_139_2', '0_2_140_2', '0_2_201_2', '0_2_202_2', '0_2_203_2', '0_2_204_2', '0_2_205_2', '0_2_206_2', '0_2_207_2', '0_2_208_2', '0_2_209_2', '0_2_210_2', '0_2_211_2', '0_2_212_2', '0_2_213_2', '0_2_214_1', '0_2_214_2', '0_2_216_2', '0_2_217_2', '0_2_218_2', '0_2_219_2', '0_2_220_2', '0_2_221_2', '0_2_222_2', '0_2_223_2', '0_2_224_2', '0_2_225_2', '0_2_226_2', '0_2_227_2', '0_2_228_2', '0_2_229_2', '0_2_230_2', '0_2_231_2', '0_2_234_2', '0_2_235_2', '0_2_241_2', '0_3_101_2', '0_3_102_2', '0_3_103_2', '0_3_104_2', '0_3_105_2', '0_3_106_2', '0_3_107_2', '0_3_108_2', '0_3_109_2', '0_3_110_2', '0_3_111_2', '0_3_112_2', '0_3_113_2', '0_3_114_2', '0_3_115_2', '0_3_116_2', '0_3_117_2', '0_3_118_2', '0_3_119_2', '0_3_120_2', '0_3_121_2', '0_3_122_2', '0_3_123_2', '0_3_124_2', '0_3_125_2', '0_3_126_2', '0_3_127_2', '0_3_128_2', '0_3_129_2', '0_3_130_2', '0_3_131_2', '0_3_135_2', '0_3_136_2', '0_3_137_2', '0_3_201_2', '0_3_202_2', '0_3_203_2', '0_3_204_2', '0_3_205_2', '0_3_206_2', '0_3_207_2', '0_3_208_2', '0_3_209_2', '0_3_210_2', '0_3_211_2', '0_3_212_2', '0_3_213_2', '0_3_214_2', '0_3_215_2', '0_3_216_2', '0_3_217_2', '0_3_218_2', '0_3_219_2', '0_3_220_2', '0_3_221_2', '0_3_222_2', '0_3_223_2', '0_3_224_2', '0_3_225_2', '0_3_226_2', '0_3_227_2', '0_3_228_2', '0_3_229_2', '0_3_230_2', '0_3_231_2', '0_3_234_2', '0_3_235_2', '0_3_236_2' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 3602, 3601, 3599, 3601, 3584, 3593, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa    
    2   0.2172872  0.2124198
   24   0.7687383  0.7677386
   46   0.7666575  0.7656499
   68   0.7624129  0.7613865
   90   0.7609483  0.7599182
  112   0.7574643  0.7564180
  134   0.7514109  0.7503379
  156   0.7538706  0.7528092
  178   0.7503479  0.7492693
  200   0.7471241  0.7460346

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was mtry = 24.


#########################################################################################


Bagged Flexible Discriminant Analysis 

3996 samples
 200 predictor
 259 classes: '0_0_102_2', '0_0_106_2', '0_0_107_2', '0_0_110_2', '0_0_111_2', '0_0_112_2', '0_0_113_2', '0_0_114_2', '0_0_115_2', '0_0_116_2', '0_0_117_2', '0_0_118_2', '0_0_119_2', '0_0_120_2', '0_0_121_2', '0_0_122_2', '0_0_123_2', '0_0_125_2', '0_0_126_2', '0_0_127_2', '0_0_128_2', '0_0_129_2', '0_0_130_2', '0_0_131_2', '0_0_132_2', '0_0_133_2', '0_0_134_2', '0_0_201_2', '0_0_202_2', '0_0_208_2', '0_0_209_2', '0_0_211_2', '0_0_212_2', '0_0_213_2', '0_0_214_2', '0_0_215_2', '0_0_216_2', '0_0_218_2', '0_0_219_2', '0_0_220_2', '0_0_222_2', '0_0_224_2', '0_0_225_2', '0_0_226_2', '0_0_227_2', '0_0_229_2', '0_0_230_2', '0_0_231_2', '0_0_232_2', '0_0_233_2', '0_0_234_2', '0_0_235_2', '0_0_236_2', '0_0_237_2', '0_1_101_2', '0_1_102_2', '0_1_103_2', '0_1_104_2', '0_1_105_2', '0_1_106_2', '0_1_107_2', '0_1_108_2', '0_1_109_2', '0_1_110_2', '0_1_111_2', '0_1_112_2', '0_1_113_2', '0_1_114_2', '0_1_115_2', '0_1_116_2', '0_1_117_2', '0_1_118_2', '0_1_119_2', '0_1_120_2', '0_1_121_2', '0_1_122_2', '0_1_123_2', '0_1_124_2', '0_1_125_2', '0_1_126_2', '0_1_127_2', '0_1_128_2', '0_1_129_2', '0_1_130_2', '0_1_136_2', '0_1_137_2', '0_1_138_2', '0_1_201_2', '0_1_202_2', '0_1_203_2', '0_1_204_2', '0_1_205_1', '0_1_205_2', '0_1_206_2', '0_1_207_2', '0_1_208_2', '0_1_209_2', '0_1_210_2', '0_1_211_2', '0_1_212_2', '0_1_213_2', '0_1_214_2', '0_1_215_2', '0_1_216_2', '0_1_217_2', '0_1_218_2', '0_1_219_2', '0_1_220_2', '0_1_221_2', '0_1_222_2', '0_1_223_2', '0_1_224_2', '0_1_225_2', '0_1_226_2', '0_1_227_2', '0_1_228_2', '0_1_229_2', '0_1_230_2', '0_1_233_2', '0_1_234_2', '0_1_235_2', '0_2_101_2', '0_2_102_2', '0_2_103_2', '0_2_104_2', '0_2_105_2', '0_2_106_2', '0_2_107_2', '0_2_108_2', '0_2_109_2', '0_2_110_2', '0_2_111_2', '0_2_112_2', '0_2_113_2', '0_2_114_2', '0_2_115_2', '0_2_117_2', '0_2_118_2', '0_2_119_2', '0_2_120_2', '0_2_121_2', '0_2_122_2', '0_2_123_2', '0_2_124_2', '0_2_125_2', '0_2_126_2', '0_2_127_2', '0_2_128_1', '0_2_128_2', '0_2_129_2', '0_2_130_2', '0_2_132_2', '0_2_133_2', '0_2_134_2', '0_2_138_2', '0_2_139_2', '0_2_140_2', '0_2_201_2', '0_2_202_2', '0_2_203_2', '0_2_204_2', '0_2_205_2', '0_2_206_2', '0_2_207_2', '0_2_208_2', '0_2_209_2', '0_2_210_2', '0_2_211_2', '0_2_212_2', '0_2_213_2', '0_2_214_1', '0_2_214_2', '0_2_216_2', '0_2_217_2', '0_2_218_2', '0_2_219_2', '0_2_220_2', '0_2_221_2', '0_2_222_2', '0_2_223_2', '0_2_224_2', '0_2_225_2', '0_2_226_2', '0_2_227_2', '0_2_228_2', '0_2_229_2', '0_2_230_2', '0_2_231_2', '0_2_234_2', '0_2_235_2', '0_2_241_2', '0_3_101_2', '0_3_102_2', '0_3_103_2', '0_3_104_2', '0_3_105_2', '0_3_106_2', '0_3_107_2', '0_3_108_2', '0_3_109_2', '0_3_110_2', '0_3_111_2', '0_3_112_2', '0_3_113_2', '0_3_114_2', '0_3_115_2', '0_3_116_2', '0_3_117_2', '0_3_118_2', '0_3_119_2', '0_3_120_2', '0_3_121_2', '0_3_122_2', '0_3_123_2', '0_3_124_2', '0_3_125_2', '0_3_126_2', '0_3_127_2', '0_3_128_2', '0_3_129_2', '0_3_130_2', '0_3_131_2', '0_3_135_2', '0_3_136_2', '0_3_137_2', '0_3_201_2', '0_3_202_2', '0_3_203_2', '0_3_204_2', '0_3_205_2', '0_3_206_2', '0_3_207_2', '0_3_208_2', '0_3_209_2', '0_3_210_2', '0_3_211_2', '0_3_212_2', '0_3_213_2', '0_3_214_2', '0_3_215_2', '0_3_216_2', '0_3_217_2', '0_3_218_2', '0_3_219_2', '0_3_220_2', '0_3_221_2', '0_3_222_2', '0_3_223_2', '0_3_224_2', '0_3_225_2', '0_3_226_2', '0_3_227_2', '0_3_228_2', '0_3_229_2', '0_3_230_2', '0_3_231_2', '0_3_234_2', '0_3_235_2', '0_3_236_2' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 3602, 3601, 3599, 3601, 3584, 3593, ... 
Resampling results across tuning parameters:

  nprune  Accuracy    Kappa     
   2      0.03027752  0.02357908
   4      0.12488064  0.12057580
   6      0.22279319  0.21951145
   8      0.31416519  0.31131744
  10      0.34995659  0.34726347
  12      0.32890184  0.32631174
  14      0.38917025  0.38666091
  16      0.39846783  0.39601144
  18      0.42529823  0.42291839
  20      0.43963226  0.43732105

Tuning parameter 'degree' was held constant at a value of 1
Accuracy was used to select the optimal model using the largest value.
The final values used for the model were degree = 1 and nprune = 20.


####################################################################################


Model Averaged Neural Network 

3996 samples
 200 predictor
 259 classes: '0_0_102_2', '0_0_106_2', '0_0_107_2', '0_0_110_2', '0_0_111_2', '0_0_112_2', '0_0_113_2', '0_0_114_2', '0_0_115_2', '0_0_116_2', '0_0_117_2', '0_0_118_2', '0_0_119_2', '0_0_120_2', '0_0_121_2', '0_0_122_2', '0_0_123_2', '0_0_125_2', '0_0_126_2', '0_0_127_2', '0_0_128_2', '0_0_129_2', '0_0_130_2', '0_0_131_2', '0_0_132_2', '0_0_133_2', '0_0_134_2', '0_0_201_2', '0_0_202_2', '0_0_208_2', '0_0_209_2', '0_0_211_2', '0_0_212_2', '0_0_213_2', '0_0_214_2', '0_0_215_2', '0_0_216_2', '0_0_218_2', '0_0_219_2', '0_0_220_2', '0_0_222_2', '0_0_224_2', '0_0_225_2', '0_0_226_2', '0_0_227_2', '0_0_229_2', '0_0_230_2', '0_0_231_2', '0_0_232_2', '0_0_233_2', '0_0_234_2', '0_0_235_2', '0_0_236_2', '0_0_237_2', '0_1_101_2', '0_1_102_2', '0_1_103_2', '0_1_104_2', '0_1_105_2', '0_1_106_2', '0_1_107_2', '0_1_108_2', '0_1_109_2', '0_1_110_2', '0_1_111_2', '0_1_112_2', '0_1_113_2', '0_1_114_2', '0_1_115_2', '0_1_116_2', '0_1_117_2', '0_1_118_2', '0_1_119_2', '0_1_120_2', '0_1_121_2', '0_1_122_2', '0_1_123_2', '0_1_124_2', '0_1_125_2', '0_1_126_2', '0_1_127_2', '0_1_128_2', '0_1_129_2', '0_1_130_2', '0_1_136_2', '0_1_137_2', '0_1_138_2', '0_1_201_2', '0_1_202_2', '0_1_203_2', '0_1_204_2', '0_1_205_1', '0_1_205_2', '0_1_206_2', '0_1_207_2', '0_1_208_2', '0_1_209_2', '0_1_210_2', '0_1_211_2', '0_1_212_2', '0_1_213_2', '0_1_214_2', '0_1_215_2', '0_1_216_2', '0_1_217_2', '0_1_218_2', '0_1_219_2', '0_1_220_2', '0_1_221_2', '0_1_222_2', '0_1_223_2', '0_1_224_2', '0_1_225_2', '0_1_226_2', '0_1_227_2', '0_1_228_2', '0_1_229_2', '0_1_230_2', '0_1_233_2', '0_1_234_2', '0_1_235_2', '0_2_101_2', '0_2_102_2', '0_2_103_2', '0_2_104_2', '0_2_105_2', '0_2_106_2', '0_2_107_2', '0_2_108_2', '0_2_109_2', '0_2_110_2', '0_2_111_2', '0_2_112_2', '0_2_113_2', '0_2_114_2', '0_2_115_2', '0_2_117_2', '0_2_118_2', '0_2_119_2', '0_2_120_2', '0_2_121_2', '0_2_122_2', '0_2_123_2', '0_2_124_2', '0_2_125_2', '0_2_126_2', '0_2_127_2', '0_2_128_1', '0_2_128_2', '0_2_129_2', '0_2_130_2', '0_2_132_2', '0_2_133_2', '0_2_134_2', '0_2_138_2', '0_2_139_2', '0_2_140_2', '0_2_201_2', '0_2_202_2', '0_2_203_2', '0_2_204_2', '0_2_205_2', '0_2_206_2', '0_2_207_2', '0_2_208_2', '0_2_209_2', '0_2_210_2', '0_2_211_2', '0_2_212_2', '0_2_213_2', '0_2_214_1', '0_2_214_2', '0_2_216_2', '0_2_217_2', '0_2_218_2', '0_2_219_2', '0_2_220_2', '0_2_221_2', '0_2_222_2', '0_2_223_2', '0_2_224_2', '0_2_225_2', '0_2_226_2', '0_2_227_2', '0_2_228_2', '0_2_229_2', '0_2_230_2', '0_2_231_2', '0_2_234_2', '0_2_235_2', '0_2_241_2', '0_3_101_2', '0_3_102_2', '0_3_103_2', '0_3_104_2', '0_3_105_2', '0_3_106_2', '0_3_107_2', '0_3_108_2', '0_3_109_2', '0_3_110_2', '0_3_111_2', '0_3_112_2', '0_3_113_2', '0_3_114_2', '0_3_115_2', '0_3_116_2', '0_3_117_2', '0_3_118_2', '0_3_119_2', '0_3_120_2', '0_3_121_2', '0_3_122_2', '0_3_123_2', '0_3_124_2', '0_3_125_2', '0_3_126_2', '0_3_127_2', '0_3_128_2', '0_3_129_2', '0_3_130_2', '0_3_131_2', '0_3_135_2', '0_3_136_2', '0_3_137_2', '0_3_201_2', '0_3_202_2', '0_3_203_2', '0_3_204_2', '0_3_205_2', '0_3_206_2', '0_3_207_2', '0_3_208_2', '0_3_209_2', '0_3_210_2', '0_3_211_2', '0_3_212_2', '0_3_213_2', '0_3_214_2', '0_3_215_2', '0_3_216_2', '0_3_217_2', '0_3_218_2', '0_3_219_2', '0_3_220_2', '0_3_221_2', '0_3_222_2', '0_3_223_2', '0_3_224_2', '0_3_225_2', '0_3_226_2', '0_3_227_2', '0_3_228_2', '0_3_229_2', '0_3_230_2', '0_3_231_2', '0_3_234_2', '0_3_235_2', '0_3_236_2' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 3602, 3601, 3599, 3601, 3584, 3593, ... 
Resampling results across tuning parameters:

  size  decay         Accuracy     Kappa       
   1    0.0000000000  0.003752873  0.0000000000
   1    0.0001000000  0.008255587  0.0013175919
   1    0.0002371374  0.008262762  0.0008966574
   1    0.0005623413  0.009776232  0.0019250531
   1    0.0013335214  0.015789584  0.0084194777
   1    0.0031622777  0.013988879  0.0062710746
   1    0.0074989421  0.014972435  0.0080223456
   1    0.0177827941  0.011257081  0.0038006126
   1    0.0421696503  0.010030740  0.0031308225
   1    0.1000000000  0.007737086  0.0008062558
   3    0.0000000000          NaN           NaN
   3    0.0001000000          NaN           NaN
   3    0.0002371374          NaN           NaN
   3    0.0005623413          NaN           NaN
   3    0.0013335214          NaN           NaN
   3    0.0031622777          NaN           NaN
   3    0.0074989421          NaN           NaN
   3    0.0177827941          NaN           NaN
   3    0.0421696503          NaN           NaN
   3    0.1000000000          NaN           NaN
   5    0.0000000000          NaN           NaN
   5    0.0001000000          NaN           NaN
   5    0.0002371374          NaN           NaN
   5    0.0005623413          NaN           NaN
   5    0.0013335214          NaN           NaN
   5    0.0031622777          NaN           NaN
   5    0.0074989421          NaN           NaN
   5    0.0177827941          NaN           NaN
   5    0.0421696503          NaN           NaN
   5    0.1000000000          NaN           NaN
   7    0.0000000000          NaN           NaN
   7    0.0001000000          NaN           NaN
   7    0.0002371374          NaN           NaN
   7    0.0005623413          NaN           NaN
   7    0.0013335214          NaN           NaN
   7    0.0031622777          NaN           NaN
   7    0.0074989421          NaN           NaN
   7    0.0177827941          NaN           NaN
   7    0.0421696503          NaN           NaN
   7    0.1000000000          NaN           NaN
   9    0.0000000000          NaN           NaN
   9    0.0001000000          NaN           NaN
   9    0.0002371374          NaN           NaN
   9    0.0005623413          NaN           NaN
   9    0.0013335214          NaN           NaN
   9    0.0031622777          NaN           NaN
   9    0.0074989421          NaN           NaN
   9    0.0177827941          NaN           NaN
   9    0.0421696503          NaN           NaN
   9    0.1000000000          NaN           NaN
  11    0.0000000000          NaN           NaN
  11    0.0001000000          NaN           NaN
  11    0.0002371374          NaN           NaN
  11    0.0005623413          NaN           NaN
  11    0.0013335214          NaN           NaN
  11    0.0031622777          NaN           NaN
  11    0.0074989421          NaN           NaN
  11    0.0177827941          NaN           NaN
  11    0.0421696503          NaN           NaN
  11    0.1000000000          NaN           NaN
  13    0.0000000000          NaN           NaN
  13    0.0001000000          NaN           NaN
  13    0.0002371374          NaN           NaN
  13    0.0005623413          NaN           NaN
  13    0.0013335214          NaN           NaN
  13    0.0031622777          NaN           NaN
  13    0.0074989421          NaN           NaN
  13    0.0177827941          NaN           NaN
  13    0.0421696503          NaN           NaN
  13    0.1000000000          NaN           NaN
  15    0.0000000000          NaN           NaN
  15    0.0001000000          NaN           NaN
  15    0.0002371374          NaN           NaN
  15    0.0005623413          NaN           NaN
  15    0.0013335214          NaN           NaN
  15    0.0031622777          NaN           NaN
  15    0.0074989421          NaN           NaN
  15    0.0177827941          NaN           NaN
  15    0.0421696503          NaN           NaN
  15    0.1000000000          NaN           NaN
  17    0.0000000000          NaN           NaN
  17    0.0001000000          NaN           NaN
  17    0.0002371374          NaN           NaN
  17    0.0005623413          NaN           NaN
  17    0.0013335214          NaN           NaN
  17    0.0031622777          NaN           NaN
  17    0.0074989421          NaN           NaN
  17    0.0177827941          NaN           NaN
  17    0.0421696503          NaN           NaN
  17    0.1000000000          NaN           NaN
  19    0.0000000000          NaN           NaN
  19    0.0001000000          NaN           NaN
  19    0.0002371374          NaN           NaN
  19    0.0005623413          NaN           NaN
  19    0.0013335214          NaN           NaN
  19    0.0031622777          NaN           NaN
  19    0.0074989421          NaN           NaN
  19    0.0177827941          NaN           NaN
  19    0.0421696503          NaN           NaN
  19    0.1000000000          NaN           NaN

Tuning parameter 'bag' was held constant at a value of FALSE
Accuracy was used to select the optimal model using the largest value.
The final values used for the model were size = 1, decay = 0.001333521 and bag = FALSE.


######################################################################################


Naive Bayes 

3996 samples
 200 predictor
 259 classes: '0_0_102_2', '0_0_106_2', '0_0_107_2', '0_0_110_2', '0_0_111_2', '0_0_112_2', '0_0_113_2', '0_0_114_2', '0_0_115_2', '0_0_116_2', '0_0_117_2', '0_0_118_2', '0_0_119_2', '0_0_120_2', '0_0_121_2', '0_0_122_2', '0_0_123_2', '0_0_125_2', '0_0_126_2', '0_0_127_2', '0_0_128_2', '0_0_129_2', '0_0_130_2', '0_0_131_2', '0_0_132_2', '0_0_133_2', '0_0_134_2', '0_0_201_2', '0_0_202_2', '0_0_208_2', '0_0_209_2', '0_0_211_2', '0_0_212_2', '0_0_213_2', '0_0_214_2', '0_0_215_2', '0_0_216_2', '0_0_218_2', '0_0_219_2', '0_0_220_2', '0_0_222_2', '0_0_224_2', '0_0_225_2', '0_0_226_2', '0_0_227_2', '0_0_229_2', '0_0_230_2', '0_0_231_2', '0_0_232_2', '0_0_233_2', '0_0_234_2', '0_0_235_2', '0_0_236_2', '0_0_237_2', '0_1_101_2', '0_1_102_2', '0_1_103_2', '0_1_104_2', '0_1_105_2', '0_1_106_2', '0_1_107_2', '0_1_108_2', '0_1_109_2', '0_1_110_2', '0_1_111_2', '0_1_112_2', '0_1_113_2', '0_1_114_2', '0_1_115_2', '0_1_116_2', '0_1_117_2', '0_1_118_2', '0_1_119_2', '0_1_120_2', '0_1_121_2', '0_1_122_2', '0_1_123_2', '0_1_124_2', '0_1_125_2', '0_1_126_2', '0_1_127_2', '0_1_128_2', '0_1_129_2', '0_1_130_2', '0_1_136_2', '0_1_137_2', '0_1_138_2', '0_1_201_2', '0_1_202_2', '0_1_203_2', '0_1_204_2', '0_1_205_1', '0_1_205_2', '0_1_206_2', '0_1_207_2', '0_1_208_2', '0_1_209_2', '0_1_210_2', '0_1_211_2', '0_1_212_2', '0_1_213_2', '0_1_214_2', '0_1_215_2', '0_1_216_2', '0_1_217_2', '0_1_218_2', '0_1_219_2', '0_1_220_2', '0_1_221_2', '0_1_222_2', '0_1_223_2', '0_1_224_2', '0_1_225_2', '0_1_226_2', '0_1_227_2', '0_1_228_2', '0_1_229_2', '0_1_230_2', '0_1_233_2', '0_1_234_2', '0_1_235_2', '0_2_101_2', '0_2_102_2', '0_2_103_2', '0_2_104_2', '0_2_105_2', '0_2_106_2', '0_2_107_2', '0_2_108_2', '0_2_109_2', '0_2_110_2', '0_2_111_2', '0_2_112_2', '0_2_113_2', '0_2_114_2', '0_2_115_2', '0_2_117_2', '0_2_118_2', '0_2_119_2', '0_2_120_2', '0_2_121_2', '0_2_122_2', '0_2_123_2', '0_2_124_2', '0_2_125_2', '0_2_126_2', '0_2_127_2', '0_2_128_1', '0_2_128_2', '0_2_129_2', '0_2_130_2', '0_2_132_2', '0_2_133_2', '0_2_134_2', '0_2_138_2', '0_2_139_2', '0_2_140_2', '0_2_201_2', '0_2_202_2', '0_2_203_2', '0_2_204_2', '0_2_205_2', '0_2_206_2', '0_2_207_2', '0_2_208_2', '0_2_209_2', '0_2_210_2', '0_2_211_2', '0_2_212_2', '0_2_213_2', '0_2_214_1', '0_2_214_2', '0_2_216_2', '0_2_217_2', '0_2_218_2', '0_2_219_2', '0_2_220_2', '0_2_221_2', '0_2_222_2', '0_2_223_2', '0_2_224_2', '0_2_225_2', '0_2_226_2', '0_2_227_2', '0_2_228_2', '0_2_229_2', '0_2_230_2', '0_2_231_2', '0_2_234_2', '0_2_235_2', '0_2_241_2', '0_3_101_2', '0_3_102_2', '0_3_103_2', '0_3_104_2', '0_3_105_2', '0_3_106_2', '0_3_107_2', '0_3_108_2', '0_3_109_2', '0_3_110_2', '0_3_111_2', '0_3_112_2', '0_3_113_2', '0_3_114_2', '0_3_115_2', '0_3_116_2', '0_3_117_2', '0_3_118_2', '0_3_119_2', '0_3_120_2', '0_3_121_2', '0_3_122_2', '0_3_123_2', '0_3_124_2', '0_3_125_2', '0_3_126_2', '0_3_127_2', '0_3_128_2', '0_3_129_2', '0_3_130_2', '0_3_131_2', '0_3_135_2', '0_3_136_2', '0_3_137_2', '0_3_201_2', '0_3_202_2', '0_3_203_2', '0_3_204_2', '0_3_205_2', '0_3_206_2', '0_3_207_2', '0_3_208_2', '0_3_209_2', '0_3_210_2', '0_3_211_2', '0_3_212_2', '0_3_213_2', '0_3_214_2', '0_3_215_2', '0_3_216_2', '0_3_217_2', '0_3_218_2', '0_3_219_2', '0_3_220_2', '0_3_221_2', '0_3_222_2', '0_3_223_2', '0_3_224_2', '0_3_225_2', '0_3_226_2', '0_3_227_2', '0_3_228_2', '0_3_229_2', '0_3_230_2', '0_3_231_2', '0_3_234_2', '0_3_235_2', '0_3_236_2' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 3602, 3601, 3599, 3601, 3584, 3593, ... 
Resampling results across tuning parameters:

  usekernel  Accuracy   Kappa    
  FALSE            NaN        NaN
   TRUE      0.4351567  0.4324263

Tuning parameter 'fL' was held constant at a value of 0
Tuning parameter 'adjust' was held constant at a value of 1
Accuracy was used to select the optimal model using the largest value.
The final values used for the model were fL = 0, usekernel = TRUE and adjust = 1.


########################################################################################


Sparse Distance Weighted Discrimination 

3996 samples
 200 predictor
 259 classes: '0_0_102_2', '0_0_106_2', '0_0_107_2', '0_0_110_2', '0_0_111_2', '0_0_112_2', '0_0_113_2', '0_0_114_2', '0_0_115_2', '0_0_116_2', '0_0_117_2', '0_0_118_2', '0_0_119_2', '0_0_120_2', '0_0_121_2', '0_0_122_2', '0_0_123_2', '0_0_125_2', '0_0_126_2', '0_0_127_2', '0_0_128_2', '0_0_129_2', '0_0_130_2', '0_0_131_2', '0_0_132_2', '0_0_133_2', '0_0_134_2', '0_0_201_2', '0_0_202_2', '0_0_208_2', '0_0_209_2', '0_0_211_2', '0_0_212_2', '0_0_213_2', '0_0_214_2', '0_0_215_2', '0_0_216_2', '0_0_218_2', '0_0_219_2', '0_0_220_2', '0_0_222_2', '0_0_224_2', '0_0_225_2', '0_0_226_2', '0_0_227_2', '0_0_229_2', '0_0_230_2', '0_0_231_2', '0_0_232_2', '0_0_233_2', '0_0_234_2', '0_0_235_2', '0_0_236_2', '0_0_237_2', '0_1_101_2', '0_1_102_2', '0_1_103_2', '0_1_104_2', '0_1_105_2', '0_1_106_2', '0_1_107_2', '0_1_108_2', '0_1_109_2', '0_1_110_2', '0_1_111_2', '0_1_112_2', '0_1_113_2', '0_1_114_2', '0_1_115_2', '0_1_116_2', '0_1_117_2', '0_1_118_2', '0_1_119_2', '0_1_120_2', '0_1_121_2', '0_1_122_2', '0_1_123_2', '0_1_124_2', '0_1_125_2', '0_1_126_2', '0_1_127_2', '0_1_128_2', '0_1_129_2', '0_1_130_2', '0_1_136_2', '0_1_137_2', '0_1_138_2', '0_1_201_2', '0_1_202_2', '0_1_203_2', '0_1_204_2', '0_1_205_1', '0_1_205_2', '0_1_206_2', '0_1_207_2', '0_1_208_2', '0_1_209_2', '0_1_210_2', '0_1_211_2', '0_1_212_2', '0_1_213_2', '0_1_214_2', '0_1_215_2', '0_1_216_2', '0_1_217_2', '0_1_218_2', '0_1_219_2', '0_1_220_2', '0_1_221_2', '0_1_222_2', '0_1_223_2', '0_1_224_2', '0_1_225_2', '0_1_226_2', '0_1_227_2', '0_1_228_2', '0_1_229_2', '0_1_230_2', '0_1_233_2', '0_1_234_2', '0_1_235_2', '0_2_101_2', '0_2_102_2', '0_2_103_2', '0_2_104_2', '0_2_105_2', '0_2_106_2', '0_2_107_2', '0_2_108_2', '0_2_109_2', '0_2_110_2', '0_2_111_2', '0_2_112_2', '0_2_113_2', '0_2_114_2', '0_2_115_2', '0_2_117_2', '0_2_118_2', '0_2_119_2', '0_2_120_2', '0_2_121_2', '0_2_122_2', '0_2_123_2', '0_2_124_2', '0_2_125_2', '0_2_126_2', '0_2_127_2', '0_2_128_1', '0_2_128_2', '0_2_129_2', '0_2_130_2', '0_2_132_2', '0_2_133_2', '0_2_134_2', '0_2_138_2', '0_2_139_2', '0_2_140_2', '0_2_201_2', '0_2_202_2', '0_2_203_2', '0_2_204_2', '0_2_205_2', '0_2_206_2', '0_2_207_2', '0_2_208_2', '0_2_209_2', '0_2_210_2', '0_2_211_2', '0_2_212_2', '0_2_213_2', '0_2_214_1', '0_2_214_2', '0_2_216_2', '0_2_217_2', '0_2_218_2', '0_2_219_2', '0_2_220_2', '0_2_221_2', '0_2_222_2', '0_2_223_2', '0_2_224_2', '0_2_225_2', '0_2_226_2', '0_2_227_2', '0_2_228_2', '0_2_229_2', '0_2_230_2', '0_2_231_2', '0_2_234_2', '0_2_235_2', '0_2_241_2', '0_3_101_2', '0_3_102_2', '0_3_103_2', '0_3_104_2', '0_3_105_2', '0_3_106_2', '0_3_107_2', '0_3_108_2', '0_3_109_2', '0_3_110_2', '0_3_111_2', '0_3_112_2', '0_3_113_2', '0_3_114_2', '0_3_115_2', '0_3_116_2', '0_3_117_2', '0_3_118_2', '0_3_119_2', '0_3_120_2', '0_3_121_2', '0_3_122_2', '0_3_123_2', '0_3_124_2', '0_3_125_2', '0_3_126_2', '0_3_127_2', '0_3_128_2', '0_3_129_2', '0_3_130_2', '0_3_131_2', '0_3_135_2', '0_3_136_2', '0_3_137_2', '0_3_201_2', '0_3_202_2', '0_3_203_2', '0_3_204_2', '0_3_205_2', '0_3_206_2', '0_3_207_2', '0_3_208_2', '0_3_209_2', '0_3_210_2', '0_3_211_2', '0_3_212_2', '0_3_213_2', '0_3_214_2', '0_3_215_2', '0_3_216_2', '0_3_217_2', '0_3_218_2', '0_3_219_2', '0_3_220_2', '0_3_221_2', '0_3_222_2', '0_3_223_2', '0_3_224_2', '0_3_225_2', '0_3_226_2', '0_3_227_2', '0_3_228_2', '0_3_229_2', '0_3_230_2', '0_3_231_2', '0_3_234_2', '0_3_235_2', '0_3_236_2' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 1 times) 
Summary of sample sizes: 3602, 3601, 3599, 3601, 3584, 3593, ... 
Resampling results across tuning parameters:

  lambda        lambda2  Accuracy     Kappa
   0.008681753  0.1      0.001999914  0    
   0.008681753  0.2      0.001999914  0    
   0.008681753  0.3      0.001999914  0    
   0.008681753  0.4      0.001999914  0    
   0.008681753  0.5      0.001999914  0    
   0.008681753  0.6      0.001999914  0    
   0.008681753  0.7      0.001999914  0    
   0.008681753  0.8      0.001999914  0    
   0.008681753  0.9      0.001999914  0    
   0.008681753  1.0      0.001999914  0    
   0.020055975  0.1      0.001999914  0    
   0.020055975  0.2      0.001999914  0    
   0.020055975  0.3      0.001999914  0    
   0.020055975  0.4      0.001999914  0    
   0.020055975  0.5      0.001999914  0    
   0.020055975  0.6      0.001999914  0    
   0.020055975  0.7      0.001999914  0    
   0.020055975  0.8      0.001999914  0    
   0.020055975  0.9      0.001999914  0    
   0.020055975  1.0      0.001999914  0    
   0.046331903  0.1      0.001999914  0    
   0.046331903  0.2      0.001999914  0    
   0.046331903  0.3      0.001999914  0    
   0.046331903  0.4      0.001999914  0    
   0.046331903  0.5      0.001999914  0    
   0.046331903  0.6      0.001999914  0    
   0.046331903  0.7      0.001999914  0    
   0.046331903  0.8      0.001999914  0    
   0.046331903  0.9      0.001999914  0    
   0.046331903  1.0      0.001999914  0    
   0.107032705  0.1      0.001999914  0    
   0.107032705  0.2      0.001999914  0    
   0.107032705  0.3      0.001999914  0    
   0.107032705  0.4      0.001999914  0    
   0.107032705  0.5      0.001999914  0    
   0.107032705  0.6      0.001999914  0    
   0.107032705  0.7      0.001999914  0    
   0.107032705  0.8      0.001999914  0    
   0.107032705  0.9      0.001999914  0    
   0.107032705  1.0      0.001999914  0    
   0.247259431  0.1      0.001999914  0    
   0.247259431  0.2      0.001999914  0    
   0.247259431  0.3      0.001999914  0    
   0.247259431  0.4      0.001999914  0    
   0.247259431  0.5      0.001999914  0    
   0.247259431  0.6      0.001999914  0    
   0.247259431  0.7      0.001999914  0    
   0.247259431  0.8      0.001999914  0    
   0.247259431  0.9      0.001999914  0    
   0.247259431  1.0      0.001999914  0    
   0.571201355  0.1      0.001999914  0    
   0.571201355  0.2      0.001999914  0    
   0.571201355  0.3      0.001999914  0    
   0.571201355  0.4      0.001999914  0    
   0.571201355  0.5      0.001999914  0    
   0.571201355  0.6      0.001999914  0    
   0.571201355  0.7      0.001999914  0    
   0.571201355  0.8      0.001999914  0    
   0.571201355  0.9      0.001999914  0    
   0.571201355  1.0      0.001999914  0    
   1.319549216  0.1      0.001999914  0    
   1.319549216  0.2      0.001999914  0    
   1.319549216  0.3      0.001999914  0    
   1.319549216  0.4      0.001999914  0    
   1.319549216  0.5      0.001999914  0    
   1.319549216  0.6      0.001999914  0    
   1.319549216  0.7      0.001999914  0    
   1.319549216  0.8      0.001999914  0    
   1.319549216  0.9      0.001999914  0    
   1.319549216  1.0      0.001999914  0    
   3.048329834  0.1      0.001999914  0    
   3.048329834  0.2      0.001999914  0    
   3.048329834  0.3      0.001999914  0    
   3.048329834  0.4      0.001999914  0    
   3.048329834  0.5      0.001999914  0    
   3.048329834  0.6      0.001999914  0    
   3.048329834  0.7      0.001999914  0    
   3.048329834  0.8      0.001999914  0    
   3.048329834  0.9      0.001999914  0    
   3.048329834  1.0      0.001999914  0    
   7.042037285  0.1      0.001999914  0    
   7.042037285  0.2      0.001999914  0    
   7.042037285  0.3      0.001999914  0    
   7.042037285  0.4      0.001999914  0    
   7.042037285  0.5      0.001999914  0    
   7.042037285  0.6      0.001999914  0    
   7.042037285  0.7      0.001999914  0    
   7.042037285  0.8      0.001999914  0    
   7.042037285  0.9      0.001999914  0    
   7.042037285  1.0      0.001999914  0    
  16.268019481  0.1      0.001999914  0    
  16.268019481  0.2      0.001999914  0    
  16.268019481  0.3      0.001999914  0    
  16.268019481  0.4      0.001999914  0    
  16.268019481  0.5      0.001999914  0    
  16.268019481  0.6      0.001999914  0    
  16.268019481  0.7      0.001999914  0    
  16.268019481  0.8      0.001999914  0    
  16.268019481  0.9      0.001999914  0    
  16.268019481  1.0      0.001999914  0    

Accuracy was used to select the optimal model using the largest value.
The final values used for the model were lambda = 16.26802 and lambda2 = 1.